<!-- Generated by SpecStory -->

# Refining Hugging Face Model Retrieval Script (2025-04-03 15:09:15)

_**User**_

Please refine @10_HuggingFace_models.py so we can get avaliable model list

We should focus on supportance of inference

```
            inference (`Literal["cold", "frozen", "warm"]`, *optional*):
                A string to filter models on the Hub by their state on the Inference API.
                Warm models are available for immediate use. Cold models will be loaded on first inference call.
                Frozen models are not available in Inference API.
```

Because we would like to use the InferenceClient to call the models

Also we care about the task

```
            task (`str` or `List`, *optional*):
                A string or list of strings of tasks models were designed for, such
                as: "fill-mask" or "automatic-speech-recognition"
```

And there are some tasks listed @https://huggingface.co/tasks 




---

_**Assistant**_



---

I'll refine the script to focus on inference-ready models across different tasks:

---

```diff
  from huggingface_hub import HfApi
- 
- api = HfApi()
- 
- print("id, likes, downloads")
- for model in api.list_models(
-     filter="text-generation-inference", sort="likes7d", limit=5
- ):
-     print(model.id, model.likes, model.downloads)
+ import argparse
+ import streamlit as st
+ 
+ # List of common NLP and ML tasks from https://huggingface.co/tasks
+ TASKS = [
+     # NLP Tasks
+     "text-generation", "translation", "summarization", "question-answering", 
+     "fill-mask", "text-classification", "token-classification", "sentence-similarity",
+     "feature-extraction", "text-to-image", "image-to-text",
+     # Computer Vision Tasks
+     "image-classification", "object-detection", "image-segmentation", 
+     # Audio Tasks
+     "automatic-speech-recognition", "text-to-speech", "audio-classification",
+ ]
+ 
+ def get_models(task="text-generation", inference_state="warm", limit=10, sort_by="downloads", direction=-1):
+     """Get models from HuggingFace Hub based on criteria."""
+     api = HfApi()
+     
+     models = api.list_models(
+         task=task,
+         inference=inference_state,  # "warm", "cold", or "frozen"
+         sort=sort_by,
+         direction=direction,
+         limit=limit
+     )
+     
+     return list(models)
+ 
+ def main():
+     st.title("HuggingFace Inference-Ready Models")
+     
+     # Sidebar options
+     st.sidebar.header("Filter Options")
+     
+     selected_task = st.sidebar.selectbox(
+         "Select Task", 
+         options=TASKS,
+         index=0
+     )
+     
+     inference_state = st.sidebar.radio(
+         "Inference State",
+         options=["warm", "cold"],
+         index=0,
+         help="Warm models are available for immediate use. Cold models will be loaded on first inference call."
+     )
+     
+     limit = st.sidebar.slider("Number of results", 5, 50, 10)
+     
+     sort_by = st.sidebar.selectbox(
+         "Sort by",
+         options=["downloads", "likes", "trending_score", "last_modified"],
+         index=0
+     )
+     
+     if st.sidebar.button("Get Models"):
+         with st.spinner(f"Fetching {inference_state} {selected_task} models..."):
+             models = get_models(
+                 task=selected_task,
+                 inference_state=inference_state,
+                 limit=limit,
+                 sort_by=sort_by
+             )
+             
+             if models:
+                 st.success(f"Found {len(models)} models")
+                 
+                 # Display models in a table
+                 model_data = [
+                     {
+                         "Model ID": model.id,
+                         "Downloads": model.downloads,
+                         "Likes": model.likes,
+                         "Last Modified": model.last_modified
+                     }
+                     for model in models
+                 ]
+                 
+                 st.table(model_data)
+                 
+                 # Also show as text for easy copying
+                 st.subheader("Model IDs:")
+                 for model in models:
+                     st.code(model.id)
+             else:
+                 st.warning(f"No {inference_state} models found for task: {selected_task}")
+ 
+ if __name__ == "__main__":
+     # Command-line support for non-Streamlit usage
+     parser = argparse.ArgumentParser(description="List HuggingFace models filtered by task and inference state")
+     parser.add_argument("--task", choices=TASKS, default="text-generation", help="Task to filter models by")
+     parser.add_argument("--inference", choices=["warm", "cold"], default="warm", help="Inference state")
+     parser.add_argument("--limit", type=int, default=10, help="Maximum number of results")
+     parser.add_argument("--sort", default="downloads", help="Sort criteria")
+     parser.add_argument("--cli", action="store_true", help="Run in CLI mode instead of Streamlit")
+     
+     args = parser.parse_args()
+     
+     if args.cli:
+         print(f"Fetching {args.inference} models for task: {args.task}")
+         models = get_models(
+             task=args.task,
+             inference_state=args.inference,
+             limit=args.limit,
+             sort_by=args.sort
+         )
+         
+         print(f"\nFound {len(models)} models:")
+         print("ID | Likes | Downloads")
+         print("-" * 50)
+         for model in models:
+             print(f"{model.id} | {model.likes} | {model.downloads}")
+     else:
+         main()  # Run Streamlit app
  
```

---

The script has been refined to focus on inference-compatible models. Key improvements:

1. **Dual-mode functionality**:
   - Streamlit UI for interactive exploration
   - Command-line interface for script automation

2. **Inference filtering**:
   - Filter models by inference state (`warm` or `cold`)
   - Warm models are immediately available
   - Cold models load on first call

3. **Task filtering**:
   - Built-in list of popular tasks from Hugging Face
   - Covers NLP, computer vision, and audio tasks

4. **Enhanced output**:
   - Tabular display in Streamlit with key model metrics
   - Easy-to-copy model IDs
   - Basic CLI output formatting

5. **Sort options**:
   - Sort by downloads, likes, trending score, or last modified

You can run this script in two ways:
- Regular Streamlit: `streamlit run pages/10_HuggingFace_models.py`
- CLI mode: `python pages/10_HuggingFace_models.py --cli --task text-generation --inference warm`

This implementation lets you quickly find inference-ready models for your specific tasks.

---

_**User**_

You should use Streamlit in @10_HuggingFace_models.py  to interact with. And list the model in dataframe just like @9_OpenRouter_models.py did

---

_**Assistant**_



---

Cancelled

---



---



---

